#!/usr/bin/env python3
"""
Debug script to test each component individually
"""

print("üîç Starting debug tests...")

# Test 1: Basic imports
print("\n1Ô∏è‚É£ Testing basic imports...")
try:
    import os
    import sys
    print("‚úÖ Basic imports OK")
except Exception as e:
    print(f"‚ùå Basic imports failed: {e}")
    exit(1)

# Test 2: FastAPI imports
print("\n2Ô∏è‚É£ Testing FastAPI imports...")
try:
    from fastapi import FastAPI, Header, HTTPException, BackgroundTasks
    from pydantic import BaseModel
    print("‚úÖ FastAPI imports OK")
except Exception as e:
    print(f"‚ùå FastAPI imports failed: {e}")
    exit(1)

# Test 3: Document processing imports
print("\n3Ô∏è‚É£ Testing document processing imports...")
try:
    from extract import extract_text_from_pdf
    print("‚úÖ Extract module OK")
except Exception as e:
    print(f"‚ùå Extract module failed: {e}")

try:
    from search import DocumentProcessor, SemanticSearch
    print("‚úÖ Search module OK")
except Exception as e:
    print(f"‚ùå Search module failed: {e}")

# Test 4: LLM processor (this is likely where the issue is)
print("\n4Ô∏è‚É£ Testing LLM processor import...")
try:
    from local_llm_processor import LLMProcessor
    print("‚úÖ LLM processor import OK")
except Exception as e:
    print(f"‚ùå LLM processor import failed: {e}")
    print(f"Error details: {type(e).__name__}: {e}")

# Test 5: Decision engine
print("\n5Ô∏è‚É£ Testing decision engine import...")
try:
    from decision_engine import DecisionEngine
    print("‚úÖ Decision engine import OK")
except Exception as e:
    print(f"‚ùå Decision engine import failed: {e}")

# Test 6: Ollama availability
print("\n6Ô∏è‚É£ Testing Ollama availability...")
try:
    import ollama
    print("‚úÖ Ollama package available")
    
    # Test if Ollama service is running
    try:
        models = ollama.list()
        print(f"‚úÖ Ollama service running. Available models: {len(models.get('models', []))}")
        
        # Check for Gemma model
        model_names = [m['name'] for m in models.get('models', [])]
        if any('gemma' in name.lower() for name in model_names):
            print("‚úÖ Gemma model found")
        else:
            print("‚ö†Ô∏è  No Gemma model found. Available models:", model_names)
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Ollama service not running: {e}")
        
except Exception as e:
    print(f"‚ùå Ollama not available: {e}")

# Test 7: Try to create LLM processor instance
print("\n7Ô∏è‚É£ Testing LLM processor initialization...")
try:
    from local_llm_processor import LLMProcessor
    processor = LLMProcessor()
    print("‚úÖ LLM processor created successfully")
except Exception as e:
    print(f"‚ùå LLM processor initialization failed: {e}")
    print(f"Error details: {type(e).__name__}: {e}")

print("\n‚úÖ Debug tests completed!")
print("\nüí° Next steps:")
print("1. Fix any ‚ùå errors shown above")
print("2. If Ollama service isn't running, start it: 'ollama serve'")
print("3. If no Gemma model, install it: 'ollama pull gemma3:12b'")
print("4. Try running app.py again")